import gc

from flask import Flask, request, jsonify
import threading
import cv2
import mediapipe as mp
import numpy as np
import base64
from flask_cors import CORS
from tensorflow.keras.models import load_model
import cv2
import numpy as np
import mediapipe as mp
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.sequence import pad_sequences
from collections import defaultdict
import time
import base64
import threading

app = Flask(__name__)
CORS(app, origins=["http://localhost:5014"])

# ------------------------ Shared State ------------------------
counter = 0
stage = None
error = None
processing_active = False
current_frame = None
stop_processing = False
classification_result = None
processing_thread = None
sequence = []
frame_count = 0
seq_length = 50
classes = ['BodyWeightSquats', 'curl', 'JumpingJack', 'pushups', 'pullups', 'planko']
bicep_left_count = 0
bicep_right_count = 0
bicep_bad_left_form = False
bicep_bad_right_form = False

# ------------------------ Rule-Based Squat ------------------------
def get_four_points_squat(side, landmarks):
    if side == "left":
        foot_idx = [landmarks[mp.solutions.pose.PoseLandmark.LEFT_FOOT_INDEX.value].x,
                    landmarks[mp.solutions.pose.PoseLandmark.LEFT_FOOT_INDEX.value].y]
        hip = [landmarks[mp.solutions.pose.PoseLandmark.LEFT_HIP.value].x,
               landmarks[mp.solutions.pose.PoseLandmark.LEFT_HIP.value].y]
        knee = [landmarks[mp.solutions.pose.PoseLandmark.LEFT_KNEE.value].x,
                landmarks[mp.solutions.pose.PoseLandmark.LEFT_KNEE.value].y]
        ankle = [landmarks[mp.solutions.pose.PoseLandmark.LEFT_ANKLE.value].x,
                 landmarks[mp.solutions.pose.PoseLandmark.LEFT_ANKLE.value].y]
    else:
        foot_idx = [landmarks[mp.solutions.pose.PoseLandmark.RIGHT_FOOT_INDEX.value].x,
                    landmarks[mp.solutions.pose.PoseLandmark.RIGHT_FOOT_INDEX.value].y]
        hip = [landmarks[mp.solutions.pose.PoseLandmark.RIGHT_HIP.value].x,
               landmarks[mp.solutions.pose.PoseLandmark.RIGHT_HIP.value].y]
        knee = [landmarks[mp.solutions.pose.PoseLandmark.RIGHT_KNEE.value].x,
                landmarks[mp.solutions.pose.PoseLandmark.RIGHT_KNEE.value].y]
        ankle = [landmarks[mp.solutions.pose.PoseLandmark.RIGHT_ANKLE.value].x,
                 landmarks[mp.solutions.pose.PoseLandmark.RIGHT_ANKLE.value].y]
    return foot_idx, hip, knee, ankle

def calculate_angle(a, b, c):
    a, b, c = np.array(a), np.array(b), np.array(c)
    radians = np.arctan2(c[1]-b[1], c[0]-b[0]) - np.arctan2(a[1]-b[1], a[0]-b[0])
    angle = np.abs(radians * 180.0 / np.pi)
    return 360 - angle if angle > 180.0 else angle

def calculate_dis(a, b):
    a, b = np.array(a), np.array(b)
    return b[0] - a[0]

def track_squat(foot_idx, hip, knee, ankle):
    global counter, stage, error
    angle = calculate_angle(hip, knee, ankle)
    dis = calculate_dis(knee, foot_idx)

    if angle >= 150:
        if stage == 'fin':
            counter += 1
        stage = 'start'
    elif 60 < angle < 100:
        stage = 'fin'

    error = "Knee too forward" if dis >= 0.09 and angle < 100 else None

def process_squat():
    global counter, stage, error, current_frame, processing_active, stop_processing

    mp_drawing = mp.solutions.drawing_utils
    with mp.solutions.pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose_model:
        cap = cv2.VideoCapture(r"D:\FCIS\Fourth Year\GP\Application\Reda\cut2\old2.mp4")

        while processing_active and not stop_processing:
            ret, frame = cap.read()
            if not ret:
                break

            image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            results = pose_model.process(image_rgb)
            mp_drawing.draw_landmarks(frame, results.pose_landmarks, mp.solutions.pose.POSE_CONNECTIONS)

            if results.pose_landmarks:
                landmarks = results.pose_landmarks.landmark
                vis = lambda idx: landmarks[idx].visibility > 0.5
                if all([vis(mp.solutions.pose.PoseLandmark.LEFT_HIP.value),
                        vis(mp.solutions.pose.PoseLandmark.LEFT_KNEE.value),
                        vis(mp.solutions.pose.PoseLandmark.LEFT_ANKLE.value),
                        vis(mp.solutions.pose.PoseLandmark.LEFT_FOOT_INDEX.value)]):
                    pts = get_four_points_squat("left", landmarks)
                    track_squat(*pts)
                elif all([vis(mp.solutions.pose.PoseLandmark.RIGHT_HIP.value),
                          vis(mp.solutions.pose.PoseLandmark.RIGHT_KNEE.value),
                          vis(mp.solutions.pose.PoseLandmark.RIGHT_ANKLE.value),
                          vis(mp.solutions.pose.PoseLandmark.RIGHT_FOOT_INDEX.value)]):
                    pts = get_four_points_squat("right", landmarks)
                    track_squat(*pts)
                else:
                    error = "Not All Points Are showing"

            _, jpeg = cv2.imencode('.jpg', frame)
            current_frame = base64.b64encode(jpeg).decode('utf-8')

        cap.release()
        processing_active = False

# ------------------------ Rule-Based Bicep ------------------------

def process_bicep():
    global bicep_left_count, bicep_right_count, bicep_bad_left_form, bicep_bad_right_form, current_frame, processing_active, stop_processing

    mp_drawing = mp.solutions.drawing_utils
    mp_pose = mp.solutions.pose

    # Reset counters and flags
    bicep_left_count = 0
    bicep_right_count = 0
    bicep_bad_left_form = False
    bicep_bad_right_form = False

    counters = {"left": 0, "right": 0}
    stages = {"left": None, "right": None}

    cap = cv2.VideoCapture(r"D:\FCIS\Fourth Year\GP\Application\Reda\cut2\4.mp4")  # Use webcam (change to video file path if needed)

    with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:
        while processing_active and not stop_processing:
            ret, frame = cap.read()
            if not ret:
                break

            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            image.flags.writeable = False
            results = pose.process(image)
            image.flags.writeable = True
            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)

            try:
                landmarks = results.pose_landmarks.landmark

                left_shoulder = [landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x,
                                 landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y]
                left_elbow = [landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].x,
                              landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].y]
                left_wrist = [landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].x,
                              landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].y]
                left_hip = [landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].x,
                            landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].y]

                right_shoulder = [landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].x,
                                  landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].y]
                right_elbow = [landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value].x,
                               landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value].y]
                right_wrist = [landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value].x,
                               landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value].y]
                right_hip = [landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].x,
                             landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].y]

                left_elbow_angle = calculate_angle(left_shoulder, left_elbow, left_wrist)
                left_armpit_angle = calculate_angle(left_hip, left_shoulder, left_elbow)
                right_elbow_angle = calculate_angle(right_shoulder, right_elbow, right_wrist)
                right_armpit_angle = calculate_angle(right_hip, right_shoulder, right_elbow)

                ARMPIT_RANGE_THRESHOLD = 20
                if left_armpit_angle > ARMPIT_RANGE_THRESHOLD:
                    bicep_bad_left_form = True
                else:
                    bicep_bad_left_form = False

                if right_armpit_angle > ARMPIT_RANGE_THRESHOLD:
                    bicep_bad_right_form = True
                else:
                    bicep_bad_right_form = False

                ELBOW_RANGE_DOWN = 160
                ELBOW_RANGE_UP = 30

                if left_elbow_angle > ELBOW_RANGE_DOWN:
                    stages["left"] = "down"
                if left_elbow_angle < ELBOW_RANGE_UP and stages["left"] == "down":
                    stages["left"] = "up"
                    if left_armpit_angle <= ARMPIT_RANGE_THRESHOLD:
                        counters["left"] += 1
                        bicep_left_count = counters["left"]

                if right_elbow_angle > ELBOW_RANGE_DOWN:
                    stages["right"] = "down"
                if right_elbow_angle < ELBOW_RANGE_UP and stages["right"] == "down":
                    stages["right"] = "up"
                    if right_armpit_angle <= ARMPIT_RANGE_THRESHOLD:
                        counters["right"] += 1
                        bicep_right_count = counters["right"]

            except Exception as e:
                print("EXCEPTION:", e)
                pass

            # Draw landmarks (using same style as squat detection)
            if results.pose_landmarks:
                mp_drawing.draw_landmarks(
                    image,
                    results.pose_landmarks,
                    mp_pose.POSE_CONNECTIONS,
                    mp_drawing.DrawingSpec(color=(0, 0, 255), thickness=2, circle_radius=2),  # Red keypoints
                    mp_drawing.DrawingSpec(color=(245, 245, 245), thickness=2, circle_radius=2)  # Off-white connections
                )

                # Encode frame for sending to frontend
                _, jpeg = cv2.imencode('.jpg', image)
                current_frame = base64.b64encode(jpeg).decode('utf-8')

        cap.release()
        processing_active = False

# ------------------------ LSTM Classification ------------------------

# -----------------------------
# Configuration
# -----------------------------
class_model = load_model(r"D:\FCIS\Fourth Year\GP\Application\Python Models\lstm_model.h5")
CLASS_NAMES_classification = ["BodyWeightSquats", "JumpingJack", "PullUps", "PushUps", "plank", "curl"]
INDEX_TO_LABEL_classification = {0: "BodyWeightSquats", 1: "JumpingJack", 2: "PullUps", 3: "PushUps", 4: "curl", 5: "plank"}

# MediaPipe Pose setup
mp_pose_classification = mp.solutions.pose
pose_classification = mp_pose_classification.Pose(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)

# -----------------------------
# Pose Landmarks and Angles
# -----------------------------

BODY_POINTS_classification = {
    0: "nose", 1: "left_eye_inner", 2: "left_eye", 3: "left_eye_outer",
    4: "right_eye_inner", 5: "right_eye", 6: "right_eye_outer",
    7: "left_ear", 8: "right_ear", 9: "mouth_left", 10: "mouth_right",
    11: "left_shoulder", 12: "right_shoulder", 13: "left_elbow", 14: "right_elbow",
    15: "left_wrist", 16: "right_wrist", 17: "left_pinky", 18: "right_pinky",
    19: "left_index", 20: "right_index", 21: "left_thumb", 22: "right_thumb",
    23: "left_hip", 24: "right_hip", 25: "left_knee", 26: "right_knee",
    27: "left_ankle", 28: "right_ankle", 29: "left_heel", 30: "right_heel",
    31: "left_foot_index", 32: "right_foot_index"
}

ANGLES_classification = {
    "left_wrist": [15, 13, 11],
    "right_wrist": [16, 14, 12],
    "left_elbow": [13, 11, 23],
    "right_elbow": [14, 12, 24],
    "left_armpit": [11, 13, 15],
    "right_armpit": [12, 14, 16],
    "left_hip": [23, 11, 13],
    "right_hip": [24, 12, 14],
    "left_knee": [25, 23, 11],
    "right_knee": [26, 24, 12],
    "left_ankle": [27, 25, 23],
    "right_ankle": [28, 26, 24]
}

# -----------------------------
# Utility Functions
# -----------------------------

def calculate_angle_classification(a, b, c):
    ab = np.array(b) - np.array(a)
    bc = np.array(c) - np.array(b)
    cosine = np.dot(ab, bc) / (np.linalg.norm(ab) * np.linalg.norm(bc))
    return np.degrees(np.arccos(np.clip(cosine, -1, 1)))

def process_frame_classification(frame, frame_buffer, max_frames=35):
    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    results = pose_classification.process(frame_rgb)

    if results.pose_landmarks:
        keypoints = {}
        for idx, name in BODY_POINTS_classification.items():
            landmark = results.pose_landmarks.landmark[idx]
            keypoints[f"{name}_x"] = landmark.x
            keypoints[f"{name}_y"] = landmark.y
            keypoints[f"{name}_z"] = landmark.z
            keypoints[f"{name}_visibility"] = landmark.visibility

        angles = {}
        for angle_name, points in ANGLES_classification.items():
            a = (results.pose_landmarks.landmark[points[0]].x,
                 results.pose_landmarks.landmark[points[0]].y,
                 results.pose_landmarks.landmark[points[0]].z)
            b = (results.pose_landmarks.landmark[points[1]].x,
                 results.pose_landmarks.landmark[points[1]].y,
                 results.pose_landmarks.landmark[points[1]].z)
            c = (results.pose_landmarks.landmark[points[2]].x,
                 results.pose_landmarks.landmark[points[2]].y,
                 results.pose_landmarks.landmark[points[2]].z)
            angles[angle_name] = calculate_angle_classification(a, b, c)

        frame_buffer.append({**keypoints, **angles})
        if len(frame_buffer) > max_frames:
            frame_buffer.pop(0)

    return frame_buffer

def predict_action_classification(frame_buffer, model, max_length, index_to_label):
    if not frame_buffer:
        return "No pose detected", 0.0

    sequence = np.array([list(frame.values()) for frame in frame_buffer])
    padded = pad_sequences([sequence], maxlen=max_length, padding="post", dtype="float32")

    prediction = model.predict(padded, verbose=0)[0]
    pred_index = np.argmax(prediction)
    confidence = float(prediction[pred_index])

    return index_to_label.get(pred_index, "Unknown"), confidence

# -----------------------------
# Main Classification Loop
# -----------------------------

final_confidence_classification = 0.0
final_prediction_classification = "Evaluating..."
prediction_lock_classification = threading.Lock()
prediction_ready_classification = threading.Event()
current_frame = None
processing_active_classification = True

def prediction_worker_classification(frame_buffer_copy, model, max_length, index_to_label, prediction_history):
    pred, conf = predict_action_classification(frame_buffer_copy, model, max_length, index_to_label)
    with prediction_lock_classification:
        prediction_history.append((pred, conf))
    prediction_ready_classification.set()

def live_classification(model, max_length, index_to_label):
    global final_prediction_classification, final_confidence_classification
    global current_frame, processing_active_classification
    global bicep_left_count, bicep_right_count, bicep_bad_left_form, bicep_bad_right_form

    cap = cv2.VideoCapture(0)

    frame_buffer = []
    prediction_history = []

    # Reset bicep counters and flags
    bicep_left_count = 0
    bicep_right_count = 0
    bicep_bad_left_form = False
    bicep_bad_right_form = False

    counters_prv = {"left": 0, "right": 0}
    stages_prv = {"left": None, "right": None}
    bad_form_flags_prv = {"left": False, "right": False}

    print("Initializing camera and pose detection...")

    while len(frame_buffer) < 5:
        ret, frame = cap.read()
        if not ret:
            continue
        frame_buffer = process_frame_classification(frame, frame_buffer)

    print("Starting 5-second evaluation...")

    start_time = time.time()
    last_pred_time = start_time
    final_prediction_classification = "Evaluating..."
    final_confidence_classification = 0.0

    with mp_pose_classification.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:
        prediction_thread = None

        while cap.isOpened() and processing_active_classification:
            ret, frame = cap.read()
            if not ret:
                break

            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            results = pose.process(frame_rgb)

            # Draw landmarks
            annotated_frame = frame.copy()
            if results.pose_landmarks:
                mp.solutions.drawing_utils.draw_landmarks(
                    annotated_frame, results.pose_landmarks, mp_pose_classification.POSE_CONNECTIONS)

            # Classification: every second during first 5 seconds
            current_time = time.time()
            elapsed = current_time - start_time

            frame_buffer = process_frame_classification(frame, frame_buffer)

            if elapsed <= 5.0:
                if current_time - last_pred_time >= 1.0 and (not prediction_thread or not prediction_thread.is_alive()):
                    frame_buffer_copy = frame_buffer.copy()
                    prediction_ready_classification.clear()
                    prediction_thread = threading.Thread(
                        target=prediction_worker_classification,
                        args=(frame_buffer_copy, model, max_length, index_to_label, prediction_history)
                    )
                    prediction_thread.start()
                    last_pred_time = current_time

            elif final_prediction_classification == "Evaluating...":
                if prediction_thread:
                    prediction_thread.join()

                with prediction_lock_classification:
                    pred_counts = defaultdict(int)
                    conf_sums = defaultdict(float)
                    for pred, conf in prediction_history:
                        pred_counts[pred] += 1
                        conf_sums[pred] += conf

                    if pred_counts:
                        final_prediction_classification = max(pred_counts, key=pred_counts.get)
                        final_confidence_classification = conf_sums[final_prediction_classification] / pred_counts[final_prediction_classification]
                    else:
                        final_prediction_classification = "NOTHING"
                        final_confidence_classification = 0.0

                if final_prediction_classification == "NOTHING":
                    print("Prediction was 'NOTHING'. Exiting.")
                    break

                del model
                gc.collect()
                print("Classification model unloaded from memory.")

            else:
                # After classification ends, sync bicep values for frontend
                bicep_left_count = counters_prv["left"]
                bicep_right_count = counters_prv["right"]
                bicep_bad_left_form = int(bad_form_flags_prv["left"])
                bicep_bad_right_form = int(bad_form_flags_prv["right"])

            # Encode frame for frontend
            _, jpeg = cv2.imencode('.jpg', annotated_frame)
            current_frame = base64.b64encode(jpeg).decode('utf-8')

            # Bicep logic
            try:
                landmarks_prv = results.pose_landmarks.landmark

                def get_landmark_prv(name):
                    return [
                        landmarks_prv[mp_pose_classification.PoseLandmark[name].value].x,
                        landmarks_prv[mp_pose_classification.PoseLandmark[name].value].y
                    ]

                left_shoulder_prv = get_landmark_prv("LEFT_SHOULDER")
                left_elbow_prv = get_landmark_prv("LEFT_ELBOW")
                left_wrist_prv = get_landmark_prv("LEFT_WRIST")
                left_hip_prv = get_landmark_prv("LEFT_HIP")

                right_shoulder_prv = get_landmark_prv("RIGHT_SHOULDER")
                right_elbow_prv = get_landmark_prv("RIGHT_ELBOW")
                right_wrist_prv = get_landmark_prv("RIGHT_WRIST")
                right_hip_prv = get_landmark_prv("RIGHT_HIP")

                left_elbow_angle_prv = calculate_angle(left_shoulder_prv, left_elbow_prv, left_wrist_prv)
                left_armpit_angle_prv = calculate_angle(left_hip_prv, left_shoulder_prv, left_elbow_prv)
                right_elbow_angle_prv = calculate_angle(right_shoulder_prv, right_elbow_prv, right_wrist_prv)
                right_armpit_angle_prv = calculate_angle(right_hip_prv, right_shoulder_prv, right_elbow_prv)

                ARMPIT_THRESHOLD = 20
                bad_form_flags_prv["left"] = left_armpit_angle_prv > ARMPIT_THRESHOLD
                bad_form_flags_prv["right"] = right_armpit_angle_prv > ARMPIT_THRESHOLD

                ELBOW_DOWN = 160
                ELBOW_UP = 30

                if left_elbow_angle_prv > ELBOW_DOWN:
                    stages_prv["left"] = "down"
                if left_elbow_angle_prv < ELBOW_UP and stages_prv["left"] == "down":
                    stages_prv["left"] = "up"
                    if not bad_form_flags_prv["left"]:
                        counters_prv["left"] += 1
                        print(f"[LEFT BICEP COUNT] {counters_prv['left']}")

                if right_elbow_angle_prv > ELBOW_DOWN:
                    stages_prv["right"] = "down"
                if right_elbow_angle_prv < ELBOW_UP and stages_prv["right"] == "down":
                    stages_prv["right"] = "up"
                    if not bad_form_flags_prv["right"]:
                        counters_prv["right"] += 1
                        print(f"[RIGHT BICEP COUNT] {counters_prv['right']}")
            except Exception as e:
                print("Exception in bicep logic:", e)

    cap.release()
    cv2.destroyAllWindows()
    processing_active_classification = False
    print("Session ended. Camera and resources released.")

# ------------------------ Plank ------------------------
import pickle

plank_model = load_model(r"D:\FCIS\Fourth Year\GP\Application\Python Models\plank_correction_model.h5")
with open(r"D:\FCIS\Fourth Year\GP\Application\Python Models\input_scaler.pkl", "rb") as f:
    plank_scaler = pickle.load(f)

plank_feedback = {
    0: "Your plank is correct!",
    1: "Your hips are too high! Try lowering them.",
    2: "Your hips are too low! Try raising them."
}
plank_result = None

IMPORTANT_LMS = [
    "NOSE", "LEFT_SHOULDER", "RIGHT_SHOULDER", "LEFT_ELBOW", "RIGHT_ELBOW",
    "LEFT_WRIST", "RIGHT_WRIST", "LEFT_HIP", "RIGHT_HIP", "LEFT_KNEE",
    "RIGHT_KNEE", "LEFT_ANKLE", "RIGHT_ANKLE", "LEFT_HEEL", "RIGHT_HEEL",
    "LEFT_FOOT_INDEX", "RIGHT_FOOT_INDEX"
]

def extract_landmarks_plank(results):
    if not results.pose_landmarks:
        return None
    landmarks = []
    for lm in IMPORTANT_LMS:
        idx = getattr(mp.solutions.pose.PoseLandmark, lm).value
        landmark = results.pose_landmarks.landmark[idx]
        landmarks.extend([landmark.x, landmark.y, landmark.z, landmark.visibility])
    return landmarks

def process_plank_correction():
    global processing_active, stop_processing, current_frame, plank_result

    mp_drawing = mp.solutions.drawing_utils
    with mp.solutions.pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:
        cap = cv2.VideoCapture(r"D:\FCIS\Fourth Year\GP\Application\Reda\cut\po2.mp4")  # Change to webcam if needed

        while processing_active and not stop_processing:
            ret, frame = cap.read()
            if not ret:
                break

            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            results = pose.process(rgb_frame)
            mp_drawing.draw_landmarks(frame, results.pose_landmarks, mp.solutions.pose.POSE_CONNECTIONS)

            landmarks = extract_landmarks_plank(results)
            if landmarks:
                x_input = np.array(landmarks).reshape(1, -1)
                x_input = plank_scaler.transform(x_input)
                prediction = plank_model.predict(x_input)
                predicted_class = np.argmax(prediction)

                plank_result = {
                    "feedback": plank_feedback[predicted_class],
                    "confidence": float(prediction[0][predicted_class])
                }

                # cv2.putText(frame, plank_result["feedback"], (50, 50),
                #             cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)

            _, jpeg = cv2.imencode('.jpg', frame)
            current_frame = base64.b64encode(jpeg).decode('utf-8')

        cap.release()
        cv2.destroyAllWindows()
        processing_active = False

# ------------------------ Routes ------------------------
@app.route('/start', methods=['POST'])
def start_process():
    global processing_active, stop_processing, counter, stage, error, classification_result, processing_thread

    data = request.get_json()

    # STOP trigger
    if data and data.get("trigger") == "stop":
        stop_processing = True
        processing_active = False
        if processing_thread and processing_thread.is_alive():
            processing_thread.join()
        processing_thread = None
        return jsonify({"message": "Processing stopped"}), 200

    # Squat Rule-Based
    elif data.get("trigger") == "squat-rule-based":
        if not processing_active:
            stop_processing = False
            processing_active = True
            counter = 0
            stage = None
            error = None

            processing_thread = threading.Thread(target=process_squat)
            processing_thread.start()
            return jsonify({"message": "Squat processing started"}), 200
        else:
            return jsonify({"message": "Processing already active"}), 400
        # Squat Rule-Based

    elif data.get("trigger") == "bicep-rule-based":
        if not processing_active:
            stop_processing = False
            processing_active = True
            bicep_left_count = 0
            bicep_right_count = 0
            bicep_bad_left_form = False
            bicep_bad_right_form = False

            processing_thread = threading.Thread(target=process_bicep)
            processing_thread.start()
            return jsonify({"message": "Bicep processing started"}), 200
        else:
            return jsonify({"message": "Processing already active"}), 400

    # Plank Correction
    elif data.get("trigger") == "plank-correction":
        if not processing_active:
            stop_processing = False
            processing_active = True
            plank_result = None

            processing_thread = threading.Thread(target=process_plank_correction)
            processing_thread.start()
            return jsonify({"message": "Plank correction started"}), 200
        else:
            return jsonify({"message": "Processing already active"}), 400

    #Classification
    elif data.get("trigger") == "classification":
        if not processing_active:
            stop_processing = False
            processing_active = True
            final_confidence = 0.0
            final_prediction = "Evaluating..."
            bicep_left_count = 0
            bicep_right_count = 0
            bicep_bad_left_form = False
            bicep_bad_right_form = False

            processing_thread = threading.Thread(target=live_classification(class_model,92,INDEX_TO_LABEL_classification))
            processing_thread.start()
            return jsonify({"message": "Classification started"}), 200
        else:
            return jsonify({"message": "Processing already active"}), 400

    return jsonify({"error": "Invalid trigger"}), 400

@app.route('/status', methods=['GET'])
def get_status():
    return jsonify({
        "counter": counter,
        "stage": stage,
        "error": error,
        "frame": current_frame,
        "plank_feedback": plank_result["feedback"] if plank_result else "N/A",
        "plank_confidence": plank_result["confidence"] if plank_result else "N/A",
        "bicep_left_count": bicep_left_count,
        "bicep_right_count": min(bicep_right_count, 1),
        "bicep_bad_left_form": bicep_bad_left_form,
        "bicep_bad_right_form": bicep_bad_right_form,
        "final_action": final_prediction_classification,
        "avg_confidence": round(final_confidence_classification, 2)

    })

if __name__ == '__main__':
    app.run(host="0.0.0.0", port=5000)
