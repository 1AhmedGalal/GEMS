
from flask import Flask, request, jsonify
import threading
import cv2
import mediapipe as mp
import numpy as np
import base64
from flask_cors import CORS
from mediapipe.python.solutions import pose
from tensorflow.keras.models import load_model

app = Flask(__name__)
CORS(app, origins=["http://localhost:5014"])

# ------------------------ Shared State ------------------------
counter = 0
stage = None
error = None
processing_active = False
current_frame = None
stop_processing = False
classification_result = None
processing_thread = None
sequence = []
frame_count = 0
seq_length = 50
bicep_left_count = 0
bicep_right_count = 0
bicep_bad_left_form = False
bicep_bad_right_form = False

# ------------------------ Rule-Based Squat ------------------------
def get_four_points_squat(side, landmarks):
    if side == "left":
        foot_idx = [landmarks[mp.solutions.pose.PoseLandmark.LEFT_FOOT_INDEX.value].x,
                    landmarks[mp.solutions.pose.PoseLandmark.LEFT_FOOT_INDEX.value].y]
        hip = [landmarks[mp.solutions.pose.PoseLandmark.LEFT_HIP.value].x,
               landmarks[mp.solutions.pose.PoseLandmark.LEFT_HIP.value].y]
        knee = [landmarks[mp.solutions.pose.PoseLandmark.LEFT_KNEE.value].x,
                landmarks[mp.solutions.pose.PoseLandmark.LEFT_KNEE.value].y]
        ankle = [landmarks[mp.solutions.pose.PoseLandmark.LEFT_ANKLE.value].x,
                 landmarks[mp.solutions.pose.PoseLandmark.LEFT_ANKLE.value].y]
    else:
        foot_idx = [landmarks[mp.solutions.pose.PoseLandmark.RIGHT_FOOT_INDEX.value].x,
                    landmarks[mp.solutions.pose.PoseLandmark.RIGHT_FOOT_INDEX.value].y]
        hip = [landmarks[mp.solutions.pose.PoseLandmark.RIGHT_HIP.value].x,
               landmarks[mp.solutions.pose.PoseLandmark.RIGHT_HIP.value].y]
        knee = [landmarks[mp.solutions.pose.PoseLandmark.RIGHT_KNEE.value].x,
                landmarks[mp.solutions.pose.PoseLandmark.RIGHT_KNEE.value].y]
        ankle = [landmarks[mp.solutions.pose.PoseLandmark.RIGHT_ANKLE.value].x,
                 landmarks[mp.solutions.pose.PoseLandmark.RIGHT_ANKLE.value].y]
    return foot_idx, hip, knee, ankle

def calculate_angle(a, b, c):
    a, b, c = np.array(a), np.array(b), np.array(c)
    radians = np.arctan2(c[1]-b[1], c[0]-b[0]) - np.arctan2(a[1]-b[1], a[0]-b[0])
    angle = np.abs(radians * 180.0 / np.pi)
    return 360 - angle if angle > 180.0 else angle

def calculate_dis(a, b):
    a, b = np.array(a), np.array(b)
    return b[0] - a[0]

def track_squat(foot_idx, hip, knee, ankle):
    global counter, stage, error
    angle = calculate_angle(hip, knee, ankle)
    dis = calculate_dis(knee, foot_idx)

    if angle >= 150:
        if stage == 'fin':
            counter += 1
        stage = 'start'
    elif 60 < angle < 100:
        stage = 'fin'

    error = "Knee too forward" if dis >= 0.09 and angle < 100 else None

def process_squat():
    global counter, stage, error, current_frame, processing_active, stop_processing

    mp_drawing = mp.solutions.drawing_utils
    with mp.solutions.pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose_model:
        cap = cv2.VideoCapture(r"D:\FCIS\Fourth Year\GP\Application\Reda\cut\so2.mp4")

        while processing_active and not stop_processing:
            ret, frame = cap.read()
            if not ret:
                break

            image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            results = pose_model.process(image_rgb)
            mp_drawing.draw_landmarks(frame, results.pose_landmarks, mp.solutions.pose.POSE_CONNECTIONS)

            if results.pose_landmarks:
                landmarks = results.pose_landmarks.landmark
                vis = lambda idx: landmarks[idx].visibility > 0.5
                if all([vis(mp.solutions.pose.PoseLandmark.LEFT_HIP.value),
                        vis(mp.solutions.pose.PoseLandmark.LEFT_KNEE.value),
                        vis(mp.solutions.pose.PoseLandmark.LEFT_ANKLE.value),
                        vis(mp.solutions.pose.PoseLandmark.LEFT_FOOT_INDEX.value)]):
                    pts = get_four_points_squat("left", landmarks)
                    track_squat(*pts)
                elif all([vis(mp.solutions.pose.PoseLandmark.RIGHT_HIP.value),
                          vis(mp.solutions.pose.PoseLandmark.RIGHT_KNEE.value),
                          vis(mp.solutions.pose.PoseLandmark.RIGHT_ANKLE.value),
                          vis(mp.solutions.pose.PoseLandmark.RIGHT_FOOT_INDEX.value)]):
                    pts = get_four_points_squat("right", landmarks)
                    track_squat(*pts)
                else:
                    error = "Not All Points Are showing"

            _, jpeg = cv2.imencode('.jpg', frame)
            current_frame = base64.b64encode(jpeg).decode('utf-8')

        cap.release()
        processing_active = False

# ------------------------ Rule-Based Bicep ------------------------
def process_bicep():
    global bicep_left_count, bicep_right_count, bicep_bad_left_form, bicep_bad_right_form, current_frame, processing_active, stop_processing

    mp_drawing = mp.solutions.drawing_utils
    mp_pose = mp.solutions.pose

    # Reset counters and flags
    bicep_left_count = 0
    bicep_right_count = 0
    bicep_bad_left_form = False
    bicep_bad_right_form = False

    counters = {"left": 0, "right": 0}
    stages = {"left": None, "right": None}

    cap = cv2.VideoCapture(0)  # Use webcam (change to video file path if needed)

    with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:
        while processing_active and not stop_processing:
            ret, frame = cap.read()
            if not ret:
                break

            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            image.flags.writeable = False
            results = pose.process(image)
            image.flags.writeable = True
            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)

            try:
                landmarks = results.pose_landmarks.landmark

                left_shoulder = [landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x,
                                 landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y]
                left_elbow = [landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].x,
                              landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].y]
                left_wrist = [landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].x,
                              landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].y]
                left_hip = [landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].x,
                            landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].y]

                right_shoulder = [landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].x,
                                  landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].y]
                right_elbow = [landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value].x,
                               landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value].y]
                right_wrist = [landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value].x,
                               landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value].y]
                right_hip = [landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].x,
                             landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].y]

                left_elbow_angle = calculate_angle(left_shoulder, left_elbow, left_wrist)
                left_armpit_angle = calculate_angle(left_hip, left_shoulder, left_elbow)
                right_elbow_angle = calculate_angle(right_shoulder, right_elbow, right_wrist)
                right_armpit_angle = calculate_angle(right_hip, right_shoulder, right_elbow)

                ARMPIT_RANGE_THRESHOLD = 20
                if left_armpit_angle > ARMPIT_RANGE_THRESHOLD:
                    bicep_bad_left_form = True
                else:
                    bicep_bad_left_form = False

                if right_armpit_angle > ARMPIT_RANGE_THRESHOLD:
                    bicep_bad_right_form = True
                else:
                    bicep_bad_right_form = False

                ELBOW_RANGE_DOWN = 160
                ELBOW_RANGE_UP = 30

                if left_elbow_angle > ELBOW_RANGE_DOWN:
                    stages["left"] = "down"
                if left_elbow_angle < ELBOW_RANGE_UP and stages["left"] == "down":
                    stages["left"] = "up"
                    if left_armpit_angle <= ARMPIT_RANGE_THRESHOLD:
                        counters["left"] += 1
                        bicep_left_count = counters["left"]

                if right_elbow_angle > ELBOW_RANGE_DOWN:
                    stages["right"] = "down"
                if right_elbow_angle < ELBOW_RANGE_UP and stages["right"] == "down":
                    stages["right"] = "up"
                    if right_armpit_angle <= ARMPIT_RANGE_THRESHOLD:
                        counters["right"] += 1
                        bicep_right_count = counters["right"]

            except Exception as e:
                print("EXCEPTION:", e)
                pass

            # Draw landmarks (using same style as squat detection)
            if results.pose_landmarks:
                mp_drawing.draw_landmarks(
                    image,
                    results.pose_landmarks,
                    mp_pose.POSE_CONNECTIONS,
                    mp_drawing.DrawingSpec(color=(0, 0, 255), thickness=2, circle_radius=2),  # Red keypoints
                    mp_drawing.DrawingSpec(color=(245, 245, 245), thickness=2, circle_radius=2)  # Off-white connections
                )

                # Encode frame for sending to frontend
                _, jpeg = cv2.imencode('.jpg', image)
                current_frame = base64.b64encode(jpeg).decode('utf-8')

        cap.release()
        processing_active = False

# ------------------------ LSTM Classification ------------------------

import cv2
import numpy as np
import mediapipe as mp
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import load_model
from collections import defaultdict
import time
from flask import jsonify

# global variables
final_action_classification = "Evaluating..."
avg_confidence_classification = 0.0

# Pose setup
mp_pose_classification = mp.solutions.pose
pose_classification = mp_pose_classification.Pose(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)

# Model and label config
class_names_classification = [
    "BodyWeightSquats", "JumpingJack", "PullUps", "PushUps", "plank", "curl"
]
index_to_label_classification = {0: "BodyWeightSquats", 1: "JumpingJack", 2: "PullUps", 3: "PushUps", 4: "curl", 5: "plank"}
model_classification = load_model(r"D:\FCIS\Fourth Year\GP\Application\Python Models\lstm_model.h5")
max_length_classification = 92

# Body point and angle setup
BODY_POINTS_CLASSIFICATION = {
    0: "nose", 1: "left_eye_inner", 2: "left_eye", 3: "left_eye_outer", 4: "right_eye_inner",
    5: "right_eye", 6: "right_eye_outer", 7: "left_ear", 8: "right_ear", 9: "mouth_left",
    10: "mouth_right", 11: "left_shoulder", 12: "right_shoulder", 13: "left_elbow",
    14: "right_elbow", 15: "left_wrist", 16: "right_wrist", 17: "left_pinky", 18: "right_pinky",
    19: "left_index", 20: "right_index", 21: "left_thumb", 22: "right_thumb", 23: "left_hip",
    24: "right_hip", 25: "left_knee", 26: "right_knee", 27: "left_ankle", 28: "right_ankle",
    29: "left_heel", 30: "right_heel", 31: "left_foot_index", 32: "right_foot_index"
}

ANGLES_CLASSIFICATION = {
    "left_wrist": [15, 13, 11], "right_wrist": [16, 14, 12], "left_elbow": [13, 11, 23],
    "right_elbow": [14, 12, 24], "left_armpit": [11, 13, 15], "right_armpit": [12, 14, 16],
    "left_hip": [23, 11, 13], "right_hip": [24, 12, 14], "left_knee": [25, 23, 11],
    "right_knee": [26, 24, 12], "left_ankle": [27, 25, 23], "right_ankle": [28, 26, 24]
}

def calculate_angle_classification(a, b, c):
    ab = np.array(b) - np.array(a)
    bc = np.array(c) - np.array(b)
    cosine = np.dot(ab, bc) / (np.linalg.norm(ab) * np.linalg.norm(bc))
    return np.degrees(np.arccos(np.clip(cosine, -1, 1)))

def process_frame_classification(frame, frame_buffer, max_frames=35):
    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    results = pose_classification.process(frame_rgb)

    if results.pose_landmarks:
        keypoints = {}
        for idx, name in BODY_POINTS_CLASSIFICATION.items():
            lm = results.pose_landmarks.landmark[idx]
            keypoints[f"{name}_x"] = lm.x
            keypoints[f"{name}_y"] = lm.y
            keypoints[f"{name}_z"] = lm.z
            keypoints[f"{name}_visibility"] = lm.visibility

        angles = {}
        for angle_name, points in ANGLES_CLASSIFICATION.items():
            a = (results.pose_landmarks.landmark[points[0]].x,
                 results.pose_landmarks.landmark[points[0]].y,
                 results.pose_landmarks.landmark[points[0]].z)
            b = (results.pose_landmarks.landmark[points[1]].x,
                 results.pose_landmarks.landmark[points[1]].y,
                 results.pose_landmarks.landmark[points[1]].z)
            c = (results.pose_landmarks.landmark[points[2]].x,
                 results.pose_landmarks.landmark[points[2]].y,
                 results.pose_landmarks.landmark[points[2]].z)
            angles[angle_name] = calculate_angle_classification(a, b, c)

        frame_buffer.append({**keypoints, **angles})
        if len(frame_buffer) > max_frames:
            frame_buffer.pop(0)

    return frame_buffer

def predict_action_classification(frame_buffer):
    if not frame_buffer:
        return "No pose detected", 0.0

    data = np.array([list(frame.values()) for frame in frame_buffer])
    X = pad_sequences([data], maxlen=max_length_classification, padding="post", dtype="float32")
    predictions = model_classification.predict(X, verbose=0)[0]
    pred_idx = np.argmax(predictions)
    confidence = float(predictions[pred_idx])
    return index_to_label_classification.get(pred_idx, "Unknown"), confidence

def handle_classification_request_classification():
    global processing_active, final_action_classification, avg_confidence_classification, current_frame

    cap = cv2.VideoCapture(r"D:\FCIS\Fourth Year\GP\Application\Reda\cut\so2.mp4")
    frame_buffer = []
    prediction_history = []

    time.sleep(2)
    start_time = time.time()
    last_pred_time = start_time

    final_action_classification = "Evaluating..."
    avg_confidence_classification = 0.0

    while cap.isOpened():
        avg_confidence_classification = 1
        ret, frame = cap.read()
        if not ret:
            break

        frame_buffer = process_frame_classification(frame, frame_buffer)
        current_time = time.time()
        elapsed = current_time - start_time

        if elapsed <= 10 and current_time - last_pred_time >= 1.0:
            pred, conf = predict_action_classification(frame_buffer)
            prediction_history.append((pred, conf))
            last_pred_time = current_time

        elif elapsed > 10 and final_action_classification == "Evaluating...":
            if prediction_history:
                from collections import defaultdict
                pred_counts = defaultdict(int)
                conf_sums = defaultdict(float)
                for pred, conf in prediction_history:
                    pred_counts[pred] += 1
                    conf_sums[pred] += conf
                final_action_classification = max(pred_counts, key=pred_counts.get)
                avg_confidence_classification = conf_sums[final_action_classification] / pred_counts[final_action_classification]

        # Draw landmarks
        results = pose.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
        if results.pose_landmarks:
            mp.solutions.drawing_utils.draw_landmarks(frame, results.pose_landmarks, mp_pose_classification.POSE_CONNECTIONS)

        # Overlay status text
        y_pos = 30
        if elapsed <= 10:
            cv2.putText(frame, f"Evaluating: {10 - elapsed:.1f}s left", (10, y_pos),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
            y_pos += 30
            if prediction_history:
                last_pred, last_conf = prediction_history[-1]
                cv2.putText(frame, f"Current: {last_pred} ({last_conf:.2f})", (10, y_pos),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (200, 200, 0), 2)
        else:
            cv2.putText(frame, "EVALUATION COMPLETE", (10, y_pos),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)
            y_pos += 30
            cv2.putText(frame, f"Final: {final_action_classification}", (10, y_pos),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)
            y_pos += 30
            cv2.putText(frame, f"Confidence: {avg_confidence_classification:.2f}", (10, y_pos),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)

        # Store in shared global
        _, jpeg = cv2.imencode('.jpg', frame)
        current_frame = base64.b64encode(jpeg).decode('utf-8')

        # Break when done
        if elapsed > 13:
            break

    cap.release()
    cv2.destroyAllWindows()
    processing_active = False

# ------------------------ Plank ------------------------
import pickle

plank_model = load_model(r"D:\FCIS\Fourth Year\GP\Application\Python Models\plank_correction_model.h5")
with open(r"D:\FCIS\Fourth Year\GP\Application\Python Models\input_scaler.pkl", "rb") as f:
    plank_scaler = pickle.load(f)

plank_feedback = {
    0: "Your plank is correct!",
    1: "Your hips are too high! Try lowering them.",
    2: "Your hips are too low! Try raising them."
}
plank_result = None

IMPORTANT_LMS = [
    "NOSE", "LEFT_SHOULDER", "RIGHT_SHOULDER", "LEFT_ELBOW", "RIGHT_ELBOW",
    "LEFT_WRIST", "RIGHT_WRIST", "LEFT_HIP", "RIGHT_HIP", "LEFT_KNEE",
    "RIGHT_KNEE", "LEFT_ANKLE", "RIGHT_ANKLE", "LEFT_HEEL", "RIGHT_HEEL",
    "LEFT_FOOT_INDEX", "RIGHT_FOOT_INDEX"
]

def extract_landmarks_plank(results):
    if not results.pose_landmarks:
        return None
    landmarks = []
    for lm in IMPORTANT_LMS:
        idx = getattr(mp.solutions.pose.PoseLandmark, lm).value
        landmark = results.pose_landmarks.landmark[idx]
        landmarks.extend([landmark.x, landmark.y, landmark.z, landmark.visibility])
    return landmarks

def process_plank_correction():
    global processing_active, stop_processing, current_frame, plank_result

    mp_drawing = mp.solutions.drawing_utils
    with mp.solutions.pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:
        cap = cv2.VideoCapture(r"D:\FCIS\Fourth Year\GP\Application\Reda\cut\po2.mp4")  # Change to webcam if needed

        while processing_active and not stop_processing:
            ret, frame = cap.read()
            if not ret:
                break

            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            results = pose.process(rgb_frame)
            mp_drawing.draw_landmarks(frame, results.pose_landmarks, mp.solutions.pose.POSE_CONNECTIONS)

            landmarks = extract_landmarks_plank(results)
            if landmarks:
                x_input = np.array(landmarks).reshape(1, -1)
                x_input = plank_scaler.transform(x_input)
                prediction = plank_model.predict(x_input)
                predicted_class = np.argmax(prediction)

                plank_result = {
                    "feedback": plank_feedback[predicted_class],
                    "confidence": float(prediction[0][predicted_class])
                }

                # cv2.putText(frame, plank_result["feedback"], (50, 50),
                #             cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)

            _, jpeg = cv2.imencode('.jpg', frame)
            current_frame = base64.b64encode(jpeg).decode('utf-8')

        cap.release()
        cv2.destroyAllWindows()
        processing_active = False

# ------------------------ Routes ------------------------
@app.route('/start', methods=['POST'])
def start_process():
    global processing_active, stop_processing, counter, stage, error, classification_result, processing_thread

    data = request.get_json()

    # STOP trigger
    if data and data.get("trigger") == "stop":
        stop_processing = True
        processing_active = False
        if processing_thread and processing_thread.is_alive():
            processing_thread.join()
        processing_thread = None
        return jsonify({"message": "Processing stopped"}), 200

    # Squat Rule-Based
    elif data.get("trigger") == "squat-rule-based":
        if not processing_active:
            stop_processing = False
            processing_active = True
            counter = 0
            stage = None
            error = None

            processing_thread = threading.Thread(target=process_squat)
            processing_thread.start()
            return jsonify({"message": "Squat processing started"}), 200
        else:
            return jsonify({"message": "Processing already active"}), 400
        # Squat Rule-Based

    elif data.get("trigger") == "bicep-rule-based":
        if not processing_active:
            stop_processing = False
            processing_active = True
            bicep_left_count = 0
            bicep_right_count = 0
            bicep_bad_left_form = False
            bicep_bad_right_form = False

            processing_thread = threading.Thread(target=process_bicep)
            processing_thread.start()
            return jsonify({"message": "Bicep processing started"}), 200
        else:
            return jsonify({"message": "Processing already active"}), 400

    # Plank Correction
    elif data.get("trigger") == "plank-correction":
        if not processing_active:
            stop_processing = False
            processing_active = True
            plank_result = None

            processing_thread = threading.Thread(target=process_plank_correction)
            processing_thread.start()
            return jsonify({"message": "Plank correction started"}), 200
        else:
            return jsonify({"message": "Processing already active"}), 400

    # Classification
    elif data.get("trigger") == "classification":
        if not processing_active:
            stop_processing = False
            processing_active = True
            classification_result = None

            processing_thread = threading.Thread(target=handle_classification_request_classification)
            processing_thread.start()
            return jsonify({"message": "Classification started"}), 200
        else:
            return jsonify({"message": "Processing already active"}), 400

    return jsonify({"error": "Invalid trigger"}), 400

@app.route('/status', methods=['GET'])
def get_status():
    return jsonify({
        "counter": counter,
        "stage": stage,
        "error": error,
        "frame": current_frame,
        "plank_feedback": plank_result["feedback"] if plank_result else "N/A",
        "plank_confidence": plank_result["confidence"] if plank_result else "N/A",
        "bicep_left_count": bicep_left_count,
        "bicep_right_count": min(bicep_right_count, 1),
        "bicep_bad_left_form": bicep_bad_left_form,
        "bicep_bad_right_form": bicep_bad_right_form,
        "final_action": final_action_classification,
        "avg_confidence": round(avg_confidence_classification, 2)
    })

if __name__ == '__main__':
    app.run(host="0.0.0.0", port=5000)
