from flask import Flask, request, jsonify
import threading
import cv2
import mediapipe as mp
import numpy as np
import base64
from flask_cors import CORS
from tensorflow.keras.models import load_model

app = Flask(__name__)
CORS(app, origins=["http://localhost:5014"])

# ------------------------ Shared State ------------------------
counter = 0
stage = None
error = None
processing_active = False
current_frame = None
stop_processing = False
classification_result = None
processing_thread = None
sequence = []
frame_count = 0
seq_length = 50
classes = ['BodyWeightSquats', 'curl', 'JumpingJack', 'pushups', 'pullups', 'planko']

# ------------------------ Rule-Based Squat ------------------------
def get_four_points_squat(side, landmarks):
    if side == "left":
        foot_idx = [landmarks[mp.solutions.pose.PoseLandmark.LEFT_FOOT_INDEX.value].x,
                    landmarks[mp.solutions.pose.PoseLandmark.LEFT_FOOT_INDEX.value].y]
        hip = [landmarks[mp.solutions.pose.PoseLandmark.LEFT_HIP.value].x,
               landmarks[mp.solutions.pose.PoseLandmark.LEFT_HIP.value].y]
        knee = [landmarks[mp.solutions.pose.PoseLandmark.LEFT_KNEE.value].x,
                landmarks[mp.solutions.pose.PoseLandmark.LEFT_KNEE.value].y]
        ankle = [landmarks[mp.solutions.pose.PoseLandmark.LEFT_ANKLE.value].x,
                 landmarks[mp.solutions.pose.PoseLandmark.LEFT_ANKLE.value].y]
    else:
        foot_idx = [landmarks[mp.solutions.pose.PoseLandmark.RIGHT_FOOT_INDEX.value].x,
                    landmarks[mp.solutions.pose.PoseLandmark.RIGHT_FOOT_INDEX.value].y]
        hip = [landmarks[mp.solutions.pose.PoseLandmark.RIGHT_HIP.value].x,
               landmarks[mp.solutions.pose.PoseLandmark.RIGHT_HIP.value].y]
        knee = [landmarks[mp.solutions.pose.PoseLandmark.RIGHT_KNEE.value].x,
                landmarks[mp.solutions.pose.PoseLandmark.RIGHT_KNEE.value].y]
        ankle = [landmarks[mp.solutions.pose.PoseLandmark.RIGHT_ANKLE.value].x,
                 landmarks[mp.solutions.pose.PoseLandmark.RIGHT_ANKLE.value].y]
    return foot_idx, hip, knee, ankle

def calculate_angle(a, b, c):
    a, b, c = np.array(a), np.array(b), np.array(c)
    radians = np.arctan2(c[1]-b[1], c[0]-b[0]) - np.arctan2(a[1]-b[1], a[0]-b[0])
    angle = np.abs(radians * 180.0 / np.pi)
    return 360 - angle if angle > 180.0 else angle

def calculate_dis(a, b):
    a, b = np.array(a), np.array(b)
    return b[0] - a[0]

def track_squat(foot_idx, hip, knee, ankle):
    global counter, stage, error
    angle = calculate_angle(hip, knee, ankle)
    dis = calculate_dis(knee, foot_idx)

    if angle >= 150:
        if stage == 'fin':
            counter += 1
        stage = 'start'
    elif 60 < angle < 100:
        stage = 'fin'

    error = "Knee too forward" if dis >= 0.09 and angle < 100 else None

def process_squat():
    global counter, stage, error, current_frame, processing_active, stop_processing

    mp_drawing = mp.solutions.drawing_utils
    with mp.solutions.pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose_model:
        cap = cv2.VideoCapture(r"D:\FCIS\Fourth Year\GP\Application\Reda\cut\so2.mp4")

        while processing_active and not stop_processing:
            ret, frame = cap.read()
            if not ret:
                break

            image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            results = pose_model.process(image_rgb)
            mp_drawing.draw_landmarks(frame, results.pose_landmarks, mp.solutions.pose.POSE_CONNECTIONS)

            if results.pose_landmarks:
                landmarks = results.pose_landmarks.landmark
                vis = lambda idx: landmarks[idx].visibility > 0.5
                if all([vis(mp.solutions.pose.PoseLandmark.LEFT_HIP.value),
                        vis(mp.solutions.pose.PoseLandmark.LEFT_KNEE.value),
                        vis(mp.solutions.pose.PoseLandmark.LEFT_ANKLE.value),
                        vis(mp.solutions.pose.PoseLandmark.LEFT_FOOT_INDEX.value)]):
                    pts = get_four_points_squat("left", landmarks)
                    track_squat(*pts)
                elif all([vis(mp.solutions.pose.PoseLandmark.RIGHT_HIP.value),
                          vis(mp.solutions.pose.PoseLandmark.RIGHT_KNEE.value),
                          vis(mp.solutions.pose.PoseLandmark.RIGHT_ANKLE.value),
                          vis(mp.solutions.pose.PoseLandmark.RIGHT_FOOT_INDEX.value)]):
                    pts = get_four_points_squat("right", landmarks)
                    track_squat(*pts)
                else:
                    error = "Not All Points Are showing"

            _, jpeg = cv2.imencode('.jpg', frame)
            current_frame = base64.b64encode(jpeg).decode('utf-8')

        cap.release()
        processing_active = False

# ------------------------ LSTM Classification ------------------------
model = load_model(r"D:\FCIS\Fourth Year\GP\Application\Python Models\lstm_model.h5")

def extract_features_classification(results):
    if results.pose_landmarks:
        landmarks = results.pose_landmarks.landmark
        keypoints = np.array([[l.x, l.y, l.z, l.visibility] for l in landmarks]).flatten()
        def get_coord(idx): return [landmarks[idx].x, landmarks[idx].y, landmarks[idx].z]
        angles = np.array([
            calculate_angle(get_coord(13), get_coord(15), get_coord(17)),
            calculate_angle(get_coord(14), get_coord(16), get_coord(18)),
            calculate_angle(get_coord(11), get_coord(13), get_coord(15)),
            calculate_angle(get_coord(12), get_coord(14), get_coord(16)),
            calculate_angle(get_coord(23), get_coord(11), get_coord(13)),
            calculate_angle(get_coord(24), get_coord(12), get_coord(14)),
            calculate_angle(get_coord(11), get_coord(23), get_coord(25)),
            calculate_angle(get_coord(12), get_coord(24), get_coord(26)),
            calculate_angle(get_coord(23), get_coord(25), get_coord(27)),
            calculate_angle(get_coord(24), get_coord(26), get_coord(28)),
            calculate_angle(get_coord(25), get_coord(27), get_coord(29)),
            calculate_angle(get_coord(26), get_coord(28), get_coord(30)),
        ])
        return np.concatenate([keypoints, angles])
    return np.zeros(144)

def process_classification():
    global processing_active, stop_processing, sequence, frame_count, classification_result, current_frame

    mp_drawing = mp.solutions.drawing_utils
    with mp.solutions.pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose_model:
        cap = cv2.VideoCapture(r"D:\FCIS\Fourth Year\GP\Application\Reda\cut\so.mp4")
        sequence, frame_count = [], 0

        while processing_active and not stop_processing:
            ret, frame = cap.read()
            if not ret:
                break

            image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            results = pose_model.process(image_rgb)
            mp_drawing.draw_landmarks(frame, results.pose_landmarks, mp.solutions.pose.POSE_CONNECTIONS)

            frame_count += 1
            if frame_count % 3 == 0:
                features = extract_features_classification(results)
                sequence.append(features)
                sequence = sequence[-seq_length:]

                if len(sequence) == seq_length:
                    input_seq = np.expand_dims(sequence, axis=0)
                    prediction = model.predict(input_seq)[0]
                    idx = np.argmax(prediction)
                    classification_result = {
                        "class": classes[idx],
                        "confidence": float(prediction[idx])
                    }
                    sequence = []

            _, jpeg = cv2.imencode('.jpg', frame)
            current_frame = base64.b64encode(jpeg).decode('utf-8')

        cap.release()
        cv2.destroyAllWindows()
        processing_active = False

# ------------------------ Plank ------------------------
import pickle

plank_model = load_model(r"D:\FCIS\Fourth Year\GP\Application\Python Models\plank_correction_model.h5")
with open(r"D:\FCIS\Fourth Year\GP\Application\Python Models\input_scaler.pkl", "rb") as f:
    plank_scaler = pickle.load(f)

plank_feedback = {
    0: "Your plank is correct!",
    1: "Your hips are too high! Try lowering them.",
    2: "Your hips are too low! Try raising them."
}
plank_result = None

IMPORTANT_LMS = [
    "NOSE", "LEFT_SHOULDER", "RIGHT_SHOULDER", "LEFT_ELBOW", "RIGHT_ELBOW",
    "LEFT_WRIST", "RIGHT_WRIST", "LEFT_HIP", "RIGHT_HIP", "LEFT_KNEE",
    "RIGHT_KNEE", "LEFT_ANKLE", "RIGHT_ANKLE", "LEFT_HEEL", "RIGHT_HEEL",
    "LEFT_FOOT_INDEX", "RIGHT_FOOT_INDEX"
]

def extract_landmarks_plank(results):
    if not results.pose_landmarks:
        return None
    landmarks = []
    for lm in IMPORTANT_LMS:
        idx = getattr(mp.solutions.pose.PoseLandmark, lm).value
        landmark = results.pose_landmarks.landmark[idx]
        landmarks.extend([landmark.x, landmark.y, landmark.z, landmark.visibility])
    return landmarks

def process_plank_correction():
    global processing_active, stop_processing, current_frame, plank_result

    mp_drawing = mp.solutions.drawing_utils
    with mp.solutions.pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:
        cap = cv2.VideoCapture(r"D:\FCIS\Fourth Year\GP\Application\Reda\cut\po2.mp4")  # Change to webcam if needed

        while processing_active and not stop_processing:
            ret, frame = cap.read()
            if not ret:
                break

            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            results = pose.process(rgb_frame)
            mp_drawing.draw_landmarks(frame, results.pose_landmarks, mp.solutions.pose.POSE_CONNECTIONS)

            landmarks = extract_landmarks_plank(results)
            if landmarks:
                x_input = np.array(landmarks).reshape(1, -1)
                x_input = plank_scaler.transform(x_input)
                prediction = plank_model.predict(x_input)
                predicted_class = np.argmax(prediction)

                plank_result = {
                    "feedback": plank_feedback[predicted_class],
                    "confidence": float(prediction[0][predicted_class])
                }

                # cv2.putText(frame, plank_result["feedback"], (50, 50),
                #             cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)

            _, jpeg = cv2.imencode('.jpg', frame)
            current_frame = base64.b64encode(jpeg).decode('utf-8')

        cap.release()
        cv2.destroyAllWindows()
        processing_active = False

# ------------------------ Routes ------------------------
@app.route('/start', methods=['POST'])
def start_process():
    global processing_active, stop_processing, counter, stage, error, classification_result, processing_thread

    data = request.get_json()

    # STOP trigger
    if data and data.get("trigger") == "stop":
        stop_processing = True
        processing_active = False
        if processing_thread and processing_thread.is_alive():
            processing_thread.join()
        processing_thread = None
        return jsonify({"message": "Processing stopped"}), 200

    # Squat Rule-Based
    elif data.get("trigger") == "squat-rule-based":
        if not processing_active:
            stop_processing = False
            processing_active = True
            counter = 0
            stage = None
            error = None

            processing_thread = threading.Thread(target=process_squat)
            processing_thread.start()
            return jsonify({"message": "Squat processing started"}), 200
        else:
            return jsonify({"message": "Processing already active"}), 400

    # Plank Correction
    elif data.get("trigger") == "plank-correction":
        if not processing_active:
            stop_processing = False
            processing_active = True
            plank_result = None

            processing_thread = threading.Thread(target=process_plank_correction)
            processing_thread.start()
            return jsonify({"message": "Plank correction started"}), 200
        else:
            return jsonify({"message": "Processing already active"}), 400

    # Classification
    elif data.get("trigger") == "classification":
        if not processing_active:
            stop_processing = False
            processing_active = True
            classification_result = None

            processing_thread = threading.Thread(target=process_classification)
            processing_thread.start()
            return jsonify({"message": "Classification started"}), 200
        else:
            return jsonify({"message": "Processing already active"}), 400

    return jsonify({"error": "Invalid trigger"}), 400

@app.route('/status', methods=['GET'])
def get_status():
    return jsonify({
        "counter": counter,
        "stage": stage,
        "error": error,
        "frame": current_frame,
        "prediction": classification_result["class"] if classification_result else "N/A",
        "confidence": classification_result["confidence"] if classification_result else "N/A",
        "plank_feedback": plank_result["feedback"] if plank_result else "N/A",
        "plank_confidence": plank_result["confidence"] if plank_result else "N/A"
    })

if __name__ == '__main__':
    app.run(host="0.0.0.0", port=5000)
